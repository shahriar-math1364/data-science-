# -*- coding: utf-8 -*-
"""Untitled36.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bcsofp0rxfuxMQycG1vCeT1LldyF4SQw
"""
import os
import gradio as gr
from huggingface_hub import Repository
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import hf_hub_download
import spaces
import torch 

# ‚úÖ Load the Hugging Face API Token from Space Secrets
hf_token = os.getenv("HF_TOKEN")  # Uses your existing token

if not hf_token:
    raise ValueError("‚ùå API token is missing! Make sure 'half_token' is added in Hugging Face Space Secrets.")

repo = Repository(
    local_dir="secret-textbooks",
    repo_type="dataset",
    clone_from="djshahriar2345/texts",  # Replace with your actual dataset name
    token=hf_token
)

repo.git_pull()


textbook_folder = "secret-textbooks/"





# ‚úÖ Load embedding model (This can utilize GPU)
embedding_model = "sentence-transformers/all-mpnet-base-v2"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model)

# ‚úÖ Lists to store text chunks and metadata
text_chunks = []
metadata_list = []

 # ‚úÖ This will run the loop using GPU if possible
def process_textbooks():
    """Reads, processes, and embeds textbooks using GPU."""
    for filename in os.listdir(textbook_folder):
        if filename.endswith(".txt"):
            book_title = filename.replace(".txt", "")  # Extract book title
            book_path = os.path.join(textbook_folder, filename)

            print(f"üìñ Processing TXT file: {filename}")

            with open(book_path, "r", encoding="utf-8") as f:
                text = f.read()

            # ‚úÖ Split text into smaller chunks
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
            chunks = text_splitter.split_text(text)

            # ‚úÖ Embed chunks in parallel (Utilizing GPU)
            with torch.no_grad():  # Avoid unnecessary gradient computation
                embedded_chunks = embeddings.embed_documents(chunks)

            # ‚úÖ Store embeddings & metadata
            text_chunks.extend(chunks)
            metadata_list.extend([{"source": book_title}] * len(chunks))

    print(f"‚úÖ Processed {len(text_chunks)} chunks.")

# ‚úÖ Run processing function
process_textbooks()

# ‚úÖ Store extracted text in ChromaDB
vectordb = Chroma.from_texts(
    texts=text_chunks,
    embedding=embeddings,
    metadatas=metadata_list,
    persist_directory="chroma_db"
)

print("‚úÖ All textbooks processed and stored in ChromaDB.")


# ‚úÖ Create a retriever
retriever = vectordb.as_retriever()




def load_model():
    """Loads the Mistral-7B model with GPU allocation first."""
    model_name = "mistralai/Mistral-7B-Instruct-v0.3"
    hf_token = os.getenv("HF_TOKEN")  # ‚úÖ Load token correctly

    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)
    model = AutoModelForCausalLM.from_pretrained(model_name, token=hf_token, device_map="auto")

    return pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=600)

# ‚úÖ Load the model before processing large text files
qa_pipeline = load_model()



@spaces.GPU
def chatbot(query):
    """Retrieves textbook passages & generates AI response with correct references."""

    retrieved_docs = retriever.get_relevant_documents(query)

    if not retrieved_docs:
        return "‚ùå No relevant textbook information found for this query."

    # ‚úÖ Extract text & references from retrieved documents
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    references = [doc.metadata.get("source", "Unknown Source") for doc in retrieved_docs]

    # ‚úÖ Remove duplicates & format book titles properly
    references = list(set(references))

    # ‚úÖ Ensure the AI explicitly sees the source
    formatted_context = "\n\n".join(
        [f"[üìñ Source: {doc.metadata.get('source', 'Unknown Source')}]\n{doc.page_content}" for doc in retrieved_docs]
    )

    # ‚úÖ Create a structured prompt
    prompt = f"""You are an expert in psychology. Answer the following question using ONLY the provided textbook sources.
Your answer should be well-structured, detailed, and clearly cite book references. Make sure you only use the provided textbook sources.

    Context:
    {formatted_context}

    Question: {query}

    Answer in detail and provide a structures response, clearly mention the refrence and bring direct quotes if necessary:
    """

    # ‚úÖ Generate AI response
    response = qa_pipeline(prompt, max_new_tokens=1000, do_sample=True, temperature=0.7)
    generated_text = response[0]['generated_text']

    # ‚úÖ Extract AI response correctly
    if "Question:" in generated_text:
        answer = generated_text.split("necessary:")[-1].strip()
    else:
        answer = generated_text.strip()  # Fallback if "Question:" is not found

    # ‚úÖ Append correct references
    formatted_references = "\nüìö References: " + ", ".join(references)
    return answer + formatted_references


iface = gr.Interface(
    fn=chatbot,
    inputs=gr.Textbox(lines=2, placeholder="Ask a question..."),
    outputs="text",
    title="üß† Psychology AI Chatbot - Learn from Famous Books",
    description=(
        "üìö **Ask any psychology-related question!** This chatbot searches through famous psychology textbooks uploaded to the app "
        "to generate well-structured answers with direct quotes and references. "
        "\n\nüí° **Why use this app?**\n"
        "- üîç **Find expert-backed answers** sourced from real books.\n"
        "- üìù **Get referenced responses** with direct quotes.\n"
        "- üéì **Learn about psychology, habits, and cognitive science!**"
        "\n\nüîπ **Simply enter your question and let the AI retrieve the best information from textbooks.**"
    ),
)

iface.launch()
